{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_20_machine_learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQtEfZ4cTwP-"
      },
      "source": [
        "1. What is the underlying concept of Support Vector Machines?\n",
        "Ans: \n",
        "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. \n",
        "The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
        "\n",
        "2. What is the concept of a support vector?\n",
        "Ans:\n",
        "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors,\n",
        "we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane.\n",
        "\n",
        "3. When using SVMs, why is it necessary to scale the inputs?\n",
        "Ans:\n",
        "Because Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal hyperplane is influenced by the scale of \n",
        "the input features and it's therefore recommended that data be standardized (mean 0, var 1) prior to SVM model training.\n",
        "\n",
        "\n",
        "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
        "Ans: \n",
        "The use of the margin of SVMs (or a non-linear transformation of the margin, for example a sigmoid as in Platt's method) as a measure of confidence, while widely used, has no support at all from the theory. In other words, things can be arbitrarily bad.\n",
        "The problem is the hinge loss. It is possible to show that when you use the hinge loss, even with an infinite amount of data, the margin will not converge to any measure of confidence or to the conditional probabilities.\n",
        "\n",
        "\n",
        "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
        "Ans: \n",
        "So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow.\n",
        "\n",
        "\n",
        "6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
        "Ans: It would be reasonable to try decreasing C. It would also be reasonable to try increasing \\sigma ^2.\n",
        "\n",
        "\n",
        "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
        "Ans: \n",
        "\n",
        "\n",
        "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
        "Ans: \n",
        "Of course it depends on the dataset and of course a lot of other factors add weight but today in this small post I’ll demonstrate how to use LinearSVC , SVC classifier and SGD classifier via Python code and also compare results for the same dataset.\n",
        "Since classifiers are very sensitive to outliers we need to scale them but before we need to pick the right dataset, for simpler results I’ll showcase the benchmark dataset for all classifiers — the Iris dataset.\n",
        "\n",
        "\n",
        "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
        "Ans: \n",
        "\n",
        "\n",
        "10. On the California housing dataset, train an SVM regressor.\n",
        "Ans:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}